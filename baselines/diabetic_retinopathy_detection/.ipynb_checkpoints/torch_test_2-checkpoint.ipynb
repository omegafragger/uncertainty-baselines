{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ce8fd34-78ab-4262-a442-6d536828c397",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-04 12:14:45.207865: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-04 12:14:45.338511: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-10-04 12:14:45.338537: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-10-04 12:14:45.372002: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-10-04 12:14:46.208395: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-10-04 12:14:46.208471: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-10-04 12:14:46.208480: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "WARNING:absl:Skipped importing the SMCalflow dataset due to ImportError. Try installing uncertainty baselines with the `datasets` extras.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jishnu/Projects/uncertainty_baselines/uncertainty_baselines/datasets/datasets.py\", line 60, in <module>\n",
      "    from uncertainty_baselines.datasets.smcalflow import MultiWoZDataset  # pylint: disable=g-import-not-at-top\n",
      "  File \"/home/jishnu/Projects/uncertainty_baselines/uncertainty_baselines/datasets/smcalflow.py\", line 40, in <module>\n",
      "    import seqio\n",
      "ModuleNotFoundError: No module named 'seqio'\n",
      "WARNING:absl:Skipped importing the Speech Commands dataset due to ImportError. Try installing uncertainty baselines with the `datasets` extras.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jishnu/Projects/uncertainty_baselines/uncertainty_baselines/datasets/datasets.py\", line 72, in <module>\n",
      "    from uncertainty_baselines.datasets.speech_commands import SpeechCommandsDataset  # pylint: disable=g-import-not-at-top\n",
      "  File \"/home/jishnu/Projects/uncertainty_baselines/uncertainty_baselines/datasets/speech_commands.py\", line 29, in <module>\n",
      "    import librosa\n",
      "ModuleNotFoundError: No module named 'librosa'\n",
      "WARNING:absl:Skipped importing the SMCalflow dataset due to ImportError. Try installing uncertainty baselines with the `datasets` extras.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jishnu/Projects/uncertainty_baselines/uncertainty_baselines/datasets/__init__.py\", line 71, in <module>\n",
      "    from uncertainty_baselines.datasets.smcalflow import MultiWoZDataset  # pylint: disable=g-import-not-at-top\n",
      "  File \"/home/jishnu/Projects/uncertainty_baselines/uncertainty_baselines/datasets/smcalflow.py\", line 40, in <module>\n",
      "    import seqio\n",
      "ModuleNotFoundError: No module named 'seqio'\n",
      "WARNING:absl:Skipped importing the Speech Commands dataset due to ImportError. Try installing uncertainty baselines with the `datasets` extras.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jishnu/Projects/uncertainty_baselines/uncertainty_baselines/datasets/__init__.py\", line 81, in <module>\n",
      "    from uncertainty_baselines.datasets.speech_commands import SpeechCommandsDataset  # pylint: disable=g-import-not-at-top\n",
      "  File \"/home/jishnu/Projects/uncertainty_baselines/uncertainty_baselines/datasets/speech_commands.py\", line 29, in <module>\n",
      "    import librosa\n",
      "ModuleNotFoundError: No module named 'librosa'\n",
      "WARNING:absl:Skipped BERT models due to ImportError.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jishnu/Projects/uncertainty_baselines/uncertainty_baselines/models/__init__.py\", line 107, in <module>\n",
      "    from uncertainty_baselines.models import bert\n",
      "  File \"/home/jishnu/Projects/uncertainty_baselines/uncertainty_baselines/models/bert.py\", line 30, in <module>\n",
      "    from official.nlp.bert import bert_models\n",
      "ModuleNotFoundError: No module named 'official.nlp.bert'\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import os\n",
    "import pathlib\n",
    "import pprint\n",
    "import time\n",
    "\n",
    "from absl import app\n",
    "from absl import flags\n",
    "from absl import logging\n",
    "import tensorflow as tf\n",
    "import uncertainty_baselines as ub\n",
    "import utils  # local file import\n",
    "import wandb\n",
    "\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "\n",
    "DEFAULT_NUM_EPOCHS = 90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "854efaad-cc5f-49c6-8ace-16d2c6d10708",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FLAGS:\n",
    "    output_dir = '/media/jishnu/Data/ub_retinopathy_outputs'\n",
    "    eyepacs_data_dir = '/media/jishnu/Data/ub_retinopathy/eyepacs'\n",
    "    aptos_data_dir = '/media/jishnu/Data/ub_retinopathy/aptos'\n",
    "    use_validation = True\n",
    "    use_test = True\n",
    "    preproc_builder_config = 'btgraham-300'\n",
    "    dr_decision_threshold = 'moderate'\n",
    "    load_from_checkpoint = False\n",
    "    checkpoint_dir = None\n",
    "    cache_eval_datasets = False\n",
    "    \n",
    "    use_wandb = False\n",
    "    wandb_dir = 'wandb'\n",
    "    project = 'ub-debug'\n",
    "    exp_name = None\n",
    "    exp_group = None\n",
    "    \n",
    "    distribution_shift = 'severity'\n",
    "    load_train_split = True\n",
    "    \n",
    "    base_learning_rate = 0.023072\n",
    "    final_decay_factor = 0.01\n",
    "    one_minus_momentum = 0.0098467\n",
    "    lr_schedule = 'step'\n",
    "    lr_warmup_epochs = 1\n",
    "    lr_decay_ratio = 0.2\n",
    "    lr_decay_epochs = ['30', '60']\n",
    "    \n",
    "    seed = 42\n",
    "    class_reweight_mode = None\n",
    "    l2 = 0.00010674\n",
    "    train_epochs = DEFAULT_NUM_EPOCHS\n",
    "    per_core_batch_size = 32\n",
    "    checkpoint_interval = 25\n",
    "    num_bins = 15\n",
    "    force_use_cpu = False\n",
    "    use_gpu = True\n",
    "    use_bfloat16 = False\n",
    "    num_cores = 1\n",
    "    tpu = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "323d230a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-04 12:14:49.776708: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "WARNING:tensorflow:There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-04 12:14:49.776789: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory\n",
      "2022-10-04 12:14:49.776864: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory\n",
      "2022-10-04 12:14:49.776926: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory\n",
      "2022-10-04 12:14:49.776989: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory\n",
      "2022-10-04 12:14:49.777062: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory\n",
      "2022-10-04 12:14:49.777127: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory\n",
      "2022-10-04 12:14:49.777189: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2022-10-04 12:14:49.777201: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2022-10-04 12:14:49.777601: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(FLAGS.seed)\n",
    "wandb_run = None\n",
    "output_dir = FLAGS.output_dir\n",
    "\n",
    "tf.io.gfile.makedirs(output_dir)\n",
    "\n",
    "# Log Run Hypers\n",
    "hypers_dict = {\n",
    "  'per_core_batch_size': FLAGS.per_core_batch_size,\n",
    "  'base_learning_rate': FLAGS.base_learning_rate,\n",
    "  'final_decay_factor': FLAGS.final_decay_factor,\n",
    "  'one_minus_momentum': FLAGS.one_minus_momentum,\n",
    "  'l2': FLAGS.l2\n",
    "}\n",
    "\n",
    "# Initialize distribution strategy on flag-specified accelerator\n",
    "strategy = utils.init_distribution_strategy(FLAGS.force_use_cpu,\n",
    "                                          FLAGS.use_gpu, FLAGS.tpu)\n",
    "use_tpu = not (FLAGS.force_use_cpu or FLAGS.use_gpu)\n",
    "per_core_batch_size = FLAGS.per_core_batch_size * FLAGS.num_cores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8004f4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_reweight_mode = FLAGS.class_reweight_mode\n",
    "if class_reweight_mode == 'constant':\n",
    "    class_weights = utils.get_diabetic_retinopathy_class_balance_weights()\n",
    "else:\n",
    "    class_weights = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3ea5b46",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-04 12:14:49.886219: I tensorflow/core/grappler/optimizers/data/replicate_on_split.cc:32] Running replicate on split optimization\n",
      "2022-10-04 12:14:49.945809: I tensorflow/core/grappler/optimizers/data/replicate_on_split.cc:32] Running replicate on split optimization\n",
      "WARNING:absl:options.experimental_threading is deprecated. Use options.threading instead.\n",
      "WARNING:absl:options.experimental_threading is deprecated. Use options.threading instead.\n",
      "2022-10-04 12:14:50.160069: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: \"RangeDataset/_3\"\n",
      "op: \"RangeDataset\"\n",
      "input: \"Const/_0\"\n",
      "input: \"Const/_1\"\n",
      "input: \"Const/_2\"\n",
      "attr {\n",
      "  key: \"_cardinality\"\n",
      "  value {\n",
      "    i: 9223372036854775807\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"metadata\"\n",
      "  value {\n",
      "    s: \"\\n\\017RangeDataset:13\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_INT64\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"replicate_on_split\"\n",
      "  value {\n",
      "    b: true\n",
      "  }\n",
      "}\n",
      "experimental_type {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_INT64\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "2022-10-04 12:14:50.219095: I tensorflow/core/grappler/optimizers/data/replicate_on_split.cc:32] Running replicate on split optimization\n",
      "2022-10-04 12:14:50.228328: I tensorflow/core/grappler/optimizers/data/replicate_on_split.cc:32] Running replicate on split optimization\n",
      "WARNING:absl:options.experimental_threading is deprecated. Use options.threading instead.\n",
      "WARNING:absl:options.experimental_threading is deprecated. Use options.threading instead.\n",
      "2022-10-04 12:14:50.268167: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: \"RangeDataset/_3\"\n",
      "op: \"RangeDataset\"\n",
      "input: \"Const/_0\"\n",
      "input: \"Const/_1\"\n",
      "input: \"Const/_2\"\n",
      "attr {\n",
      "  key: \"_cardinality\"\n",
      "  value {\n",
      "    i: 9223372036854775807\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"metadata\"\n",
      "  value {\n",
      "    s: \"\\n\\017RangeDataset:38\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_INT64\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"replicate_on_split\"\n",
      "  value {\n",
      "    b: true\n",
      "  }\n",
      "}\n",
      "experimental_type {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_INT64\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "2022-10-04 12:14:50.320402: I tensorflow/core/grappler/optimizers/data/replicate_on_split.cc:32] Running replicate on split optimization\n",
      "2022-10-04 12:14:50.329905: I tensorflow/core/grappler/optimizers/data/replicate_on_split.cc:32] Running replicate on split optimization\n",
      "WARNING:absl:options.experimental_threading is deprecated. Use options.threading instead.\n",
      "WARNING:absl:options.experimental_threading is deprecated. Use options.threading instead.\n",
      "2022-10-04 12:14:50.372410: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: \"RangeDataset/_3\"\n",
      "op: \"RangeDataset\"\n",
      "input: \"Const/_0\"\n",
      "input: \"Const/_1\"\n",
      "input: \"Const/_2\"\n",
      "attr {\n",
      "  key: \"_cardinality\"\n",
      "  value {\n",
      "    i: 9223372036854775807\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"metadata\"\n",
      "  value {\n",
      "    s: \"\\n\\017RangeDataset:63\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_INT64\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"replicate_on_split\"\n",
      "  value {\n",
      "    b: true\n",
      "  }\n",
      "}\n",
      "experimental_type {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_INT64\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "2022-10-04 12:14:50.425477: I tensorflow/core/grappler/optimizers/data/replicate_on_split.cc:32] Running replicate on split optimization\n",
      "2022-10-04 12:14:50.435103: I tensorflow/core/grappler/optimizers/data/replicate_on_split.cc:32] Running replicate on split optimization\n",
      "WARNING:absl:options.experimental_threading is deprecated. Use options.threading instead.\n",
      "WARNING:absl:options.experimental_threading is deprecated. Use options.threading instead.\n",
      "2022-10-04 12:14:50.478811: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: \"RangeDataset/_3\"\n",
      "op: \"RangeDataset\"\n",
      "input: \"Const/_0\"\n",
      "input: \"Const/_1\"\n",
      "input: \"Const/_2\"\n",
      "attr {\n",
      "  key: \"_cardinality\"\n",
      "  value {\n",
      "    i: 9223372036854775807\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"metadata\"\n",
      "  value {\n",
      "    s: \"\\n\\017RangeDataset:88\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_INT64\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"replicate_on_split\"\n",
      "  value {\n",
      "    b: true\n",
      "  }\n",
      "}\n",
      "experimental_type {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_INT64\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "2022-10-04 12:14:50.537155: I tensorflow/core/grappler/optimizers/data/replicate_on_split.cc:32] Running replicate on split optimization\n",
      "2022-10-04 12:14:50.547122: I tensorflow/core/grappler/optimizers/data/replicate_on_split.cc:32] Running replicate on split optimization\n",
      "WARNING:absl:options.experimental_threading is deprecated. Use options.threading instead.\n",
      "WARNING:absl:options.experimental_threading is deprecated. Use options.threading instead.\n",
      "2022-10-04 12:14:50.591788: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: \"RangeDataset/_3\"\n",
      "op: \"RangeDataset\"\n",
      "input: \"Const/_0\"\n",
      "input: \"Const/_1\"\n",
      "input: \"Const/_2\"\n",
      "attr {\n",
      "  key: \"_cardinality\"\n",
      "  value {\n",
      "    i: 9223372036854775807\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"metadata\"\n",
      "  value {\n",
      "    s: \"\\n\\020RangeDataset:113\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_INT64\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"replicate_on_split\"\n",
      "  value {\n",
      "    b: true\n",
      "  }\n",
      "}\n",
      "experimental_type {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_INT64\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datasets, steps = utils.load_dataset(\n",
    "  train_batch_size=per_core_batch_size,\n",
    "  eval_batch_size=per_core_batch_size,\n",
    "  flags=FLAGS,\n",
    "  strategy=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f9d02d87",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-04 12:14:50.705973: W tensorflow/core/framework/dataset.cc:769] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    }
   ],
   "source": [
    "dataset_iterators = {\n",
    "    'train': iter(datasets['train']),\n",
    "    'in_domain_validation': iter(datasets['in_domain_validation']),\n",
    "    'ood_validation': iter(datasets['ood_validation']),\n",
    "    'in_domain_test': iter(datasets['in_domain_test']),\n",
    "    'ood_test': iter(datasets['ood_test'])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "443edc96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF========================>\n",
      "tf.Tensor([ 32 512 512   3], shape=(4,), dtype=int32)\n",
      "tf.Tensor([32], shape=(1,), dtype=int32)\n",
      "==========================>\n",
      "torch.Size([32, 3, 512, 512])\n",
      "torch.Size([32])\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(1., device='cuda:0')\n",
      "tensor([0.0000e+00, 2.5132e-06, 4.3383e-06,  ..., 1.0000e+00, 1.0000e+00,\n",
      "        1.0000e+00], device='cuda:0')\n",
      "tensor([0., 1.], device='cuda:0')\n",
      "TF========================>\n",
      "tf.Tensor([ 32 512 512   3], shape=(4,), dtype=int32)\n",
      "tf.Tensor([32], shape=(1,), dtype=int32)\n",
      "==========================>\n",
      "torch.Size([32, 3, 512, 512])\n",
      "torch.Size([32])\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(1., device='cuda:0')\n",
      "tensor([0.0000e+00, 8.9758e-07, 1.3464e-06,  ..., 1.0000e+00, 1.0000e+00,\n",
      "        1.0000e+00], device='cuda:0')\n",
      "tensor([0., 1.], device='cuda:0')\n",
      "TF========================>\n",
      "tf.Tensor([ 32 512 512   3], shape=(4,), dtype=int32)\n",
      "tf.Tensor([32], shape=(1,), dtype=int32)\n",
      "==========================>\n",
      "torch.Size([32, 3, 512, 512])\n",
      "torch.Size([32])\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(1., device='cuda:0')\n",
      "tensor([0.0000e+00, 4.1139e-06, 5.3855e-06,  ..., 1.0000e+00, 1.0000e+00,\n",
      "        1.0000e+00], device='cuda:0')\n",
      "tensor([1.], device='cuda:0')\n",
      "TF========================>\n",
      "tf.Tensor([ 32 512 512   3], shape=(4,), dtype=int32)\n",
      "tf.Tensor([32], shape=(1,), dtype=int32)\n",
      "==========================>\n",
      "torch.Size([32, 3, 512, 512])\n",
      "torch.Size([32])\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(1., device='cuda:0')\n",
      "tensor([0.0000e+00, 8.9758e-08, 8.9758e-07,  ..., 1.0000e+00, 1.0000e+00,\n",
      "        1.0000e+00], device='cuda:0')\n",
      "tensor([0., 1.], device='cuda:0')\n",
      "TF========================>\n",
      "tf.Tensor([ 32 512 512   3], shape=(4,), dtype=int32)\n",
      "tf.Tensor([32], shape=(1,), dtype=int32)\n",
      "==========================>\n",
      "torch.Size([32, 3, 512, 512])\n",
      "torch.Size([32])\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(1., device='cuda:0')\n",
      "tensor([0.0000e+00, 4.7272e-06, 7.0610e-06,  ..., 1.0000e+00, 1.0000e+00,\n",
      "        1.0000e+00], device='cuda:0')\n",
      "tensor([1.], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def get_torch_inputs(inputs, batch_size, image_h, image_w, device):\n",
    "    images = inputs['features']\n",
    "    labels = inputs['labels']\n",
    "    print ('TF========================>')\n",
    "    print (tf.shape(images))\n",
    "    print (tf.shape(labels))\n",
    "    print ('==========================>')\n",
    "    images = torch.from_numpy(images._numpy()).view(batch_size, 3, image_h, image_w).to(device)\n",
    "    labels = torch.from_numpy(labels._numpy()).to(device).float()\n",
    "    return images, labels\n",
    "\n",
    "# Train\n",
    "image_h = 512\n",
    "image_w = 512\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# iD Train\n",
    "for step in range(steps['train']):\n",
    "    inputs = next(dataset_iterators['train'])\n",
    "    images, labels = get_torch_inputs(inputs, per_core_batch_size, image_h, image_w, device)\n",
    "    images = images.to(device)\n",
    "    labels = labels.to(device)\n",
    "    print (images.shape)\n",
    "    print (labels.shape)\n",
    "    print (images.min())\n",
    "    print (images.max())\n",
    "    print (images.unique())\n",
    "    print (labels.unique())\n",
    "    break\n",
    "    \n",
    "# iD val\n",
    "for step in range(steps['in_domain_validation']):\n",
    "    inputs = next(dataset_iterators['in_domain_validation'])\n",
    "    images, labels = get_torch_inputs(inputs, per_core_batch_size, image_h, image_w, device)\n",
    "    print (images.shape)\n",
    "    print (labels.shape)\n",
    "    print (images.min())\n",
    "    print (images.max())\n",
    "    print (images.unique())\n",
    "    print (labels.unique())\n",
    "    break\n",
    "\n",
    "# OoD Val\n",
    "for step in range(steps['ood_validation']):\n",
    "    inputs = next(dataset_iterators['ood_validation'])\n",
    "    images, labels = get_torch_inputs(inputs, per_core_batch_size, image_h, image_w, device)\n",
    "    print (images.shape)\n",
    "    print (labels.shape)\n",
    "    print (images.min())\n",
    "    print (images.max())\n",
    "    print (images.unique())\n",
    "    print (labels.unique())\n",
    "    break\n",
    "    \n",
    "# iD test\n",
    "for step in range(steps['in_domain_test']):\n",
    "    inputs = next(dataset_iterators['in_domain_test'])\n",
    "    images, labels = get_torch_inputs(inputs, per_core_batch_size, image_h, image_w, device)\n",
    "    print (images.shape)\n",
    "    print (labels.shape)\n",
    "    print (images.min())\n",
    "    print (images.max())\n",
    "    print (images.unique())\n",
    "    print (labels.unique())\n",
    "    break\n",
    "\n",
    "# OoD test\n",
    "for step in range(steps['ood_test']):\n",
    "    inputs = next(dataset_iterators['ood_test'])\n",
    "    images, labels = get_torch_inputs(inputs, per_core_batch_size, image_h, image_w, device)\n",
    "    print (images.shape)\n",
    "    print (labels.shape)\n",
    "    print (images.min())\n",
    "    print (images.max())\n",
    "    print (images.unique())\n",
    "    print (labels.unique())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e5c4753",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "per_core_batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cabd827",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|████████████████████████████████████████▌                                                                                                                                                                                                  | 181/1048 [01:07<00:56, 15.25it/s]"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# iD Train\n",
    "for step in tqdm(range(steps['train'])):\n",
    "    inputs = next(dataset_iterators['train'])\n",
    "    images, labels = get_torch_inputs(inputs, per_core_batch_size, image_h, image_w, device)\n",
    "    images = images.to(device)\n",
    "    labels = labels.to(device)\n",
    "    pass\n",
    "    \n",
    "# iD val\n",
    "for step in tqdm(range(steps['in_domain_validation'])):\n",
    "    inputs = next(dataset_iterators['in_domain_validation'])\n",
    "    images, labels = get_torch_inputs(inputs, per_core_batch_size, image_h, image_w, device)\n",
    "    images = images.to(device)\n",
    "    labels = labels.to(device)\n",
    "    pass\n",
    "\n",
    "# OoD Val\n",
    "for step in tqdm(range(steps['ood_validation'])):\n",
    "    inputs = next(dataset_iterators['ood_validation'])\n",
    "    images, labels = get_torch_inputs(inputs, per_core_batch_size, image_h, image_w, device)\n",
    "    images = images.to(device)\n",
    "    labels = labels.to(device)\n",
    "    pass\n",
    "    \n",
    "# iD test\n",
    "for step in tqdm(range(steps['in_domain_test'])):\n",
    "    inputs = next(dataset_iterators['in_domain_test'])\n",
    "    images, labels = get_torch_inputs(inputs, per_core_batch_size, image_h, image_w, device)\n",
    "    images = images.to(device)\n",
    "    labels = labels.to(device)\n",
    "    pass\n",
    "\n",
    "# OoD test\n",
    "for step in tqdm(range(steps['ood_test'])):\n",
    "    inputs = next(dataset_iterators['ood_test'])\n",
    "    images, labels = get_torch_inputs(inputs, per_core_batch_size, image_h, image_w, device)\n",
    "    images = images.to(device)\n",
    "    labels = labels.to(device)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a06bd45a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-03 17:52:41.413855: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-03 17:52:41.546889: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-10-03 17:52:41.546909: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-10-03 17:52:41.577213: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-10-03 17:52:42.578279: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-10-03 17:52:42.578344: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-10-03 17:52:42.578352: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "WARNING:absl:Skipped importing the SMCalflow dataset due to ImportError. Try installing uncertainty baselines with the `datasets` extras.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jishnu/Projects/uncertainty_baselines/uncertainty_baselines/datasets/datasets.py\", line 60, in <module>\n",
      "    from uncertainty_baselines.datasets.smcalflow import MultiWoZDataset  # pylint: disable=g-import-not-at-top\n",
      "  File \"/home/jishnu/Projects/uncertainty_baselines/uncertainty_baselines/datasets/smcalflow.py\", line 40, in <module>\n",
      "    import seqio\n",
      "ModuleNotFoundError: No module named 'seqio'\n",
      "WARNING:absl:Skipped importing the Speech Commands dataset due to ImportError. Try installing uncertainty baselines with the `datasets` extras.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jishnu/Projects/uncertainty_baselines/uncertainty_baselines/datasets/datasets.py\", line 72, in <module>\n",
      "    from uncertainty_baselines.datasets.speech_commands import SpeechCommandsDataset  # pylint: disable=g-import-not-at-top\n",
      "  File \"/home/jishnu/Projects/uncertainty_baselines/uncertainty_baselines/datasets/speech_commands.py\", line 29, in <module>\n",
      "    import librosa\n",
      "ModuleNotFoundError: No module named 'librosa'\n",
      "WARNING:absl:Skipped importing the SMCalflow dataset due to ImportError. Try installing uncertainty baselines with the `datasets` extras.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jishnu/Projects/uncertainty_baselines/uncertainty_baselines/datasets/__init__.py\", line 71, in <module>\n",
      "    from uncertainty_baselines.datasets.smcalflow import MultiWoZDataset  # pylint: disable=g-import-not-at-top\n",
      "  File \"/home/jishnu/Projects/uncertainty_baselines/uncertainty_baselines/datasets/smcalflow.py\", line 40, in <module>\n",
      "    import seqio\n",
      "ModuleNotFoundError: No module named 'seqio'\n",
      "WARNING:absl:Skipped importing the Speech Commands dataset due to ImportError. Try installing uncertainty baselines with the `datasets` extras.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jishnu/Projects/uncertainty_baselines/uncertainty_baselines/datasets/__init__.py\", line 81, in <module>\n",
      "    from uncertainty_baselines.datasets.speech_commands import SpeechCommandsDataset  # pylint: disable=g-import-not-at-top\n",
      "  File \"/home/jishnu/Projects/uncertainty_baselines/uncertainty_baselines/datasets/speech_commands.py\", line 29, in <module>\n",
      "    import librosa\n",
      "ModuleNotFoundError: No module named 'librosa'\n",
      "WARNING:absl:Skipped BERT models due to ImportError.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jishnu/Projects/uncertainty_baselines/uncertainty_baselines/models/__init__.py\", line 107, in <module>\n",
      "    from uncertainty_baselines.models import bert\n",
      "  File \"/home/jishnu/Projects/uncertainty_baselines/uncertainty_baselines/models/bert.py\", line 30, in <module>\n",
      "    from official.nlp.bert import bert_models\n",
      "ModuleNotFoundError: No module named 'official.nlp.bert'\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "Needs to be updated for APTOS / Severity shifts, different decision thresholds (Mild / Moderate classifiers).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_diabetic_retinopathy_class_balance_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projects/uncertainty_baselines/baselines/diabetic_retinopathy_detection/utils/loss_utils.py:67\u001b[0m, in \u001b[0;36mget_diabetic_retinopathy_class_balance_weights\u001b[0;34m(positive_empirical_prob)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Class weights used for rebalancing the dataset, by skewing the `loss`.\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \n\u001b[1;32m     37\u001b[0m \u001b[38;5;124;03mDiabetic Retinopathy positive class proportions are imbalanced:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;124;03m  Reweighted positive and negative example probabilities.\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m positive_empirical_prob \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     66\u001b[0m   \u001b[38;5;66;03m# positive_empirical_prob = 0.196\u001b[39;00m\n\u001b[0;32m---> 67\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[1;32m     68\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNeeds to be updated for APTOS / Severity shifts, \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     69\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferent decision thresholds (Mild / Moderate classifiers).\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;241m0\u001b[39m: (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m positive_empirical_prob)),\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;241m1\u001b[39m: (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m positive_empirical_prob)\n\u001b[1;32m     74\u001b[0m }\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: Needs to be updated for APTOS / Severity shifts, different decision thresholds (Mild / Moderate classifiers)."
     ]
    }
   ],
   "source": [
    "import utils\n",
    "utils.get_diabetic_retinopathy_class_balance_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92f1bbe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
